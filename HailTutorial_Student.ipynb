{"cells":[{"cell_type":"markdown","source":["# Hail Tutorial - Student Edition"],"metadata":{}},{"cell_type":"markdown","source":["### Acquring and Installing the Resources\nFirstly, we need to acquire the resources necessary to run Hail. Click the two links below to download the Hail jar and python files to your local system.\n\n* https://github.com/mptrepanier/spark-saturday-advanced/blob/master/resources/hail-2.1.1.jar?raw=true\n\n* https://github.com/mptrepanier/spark-saturday-advanced/blob/master/resources/pyhail.zip?raw=true\n\nNext, we need add these libraries to our cluster. In order to do so:\n\n1) Click the \"Workspace\" button on the left.\n\n2) On the top of the tab that opens up, click the \"Workspace\" dropdown.\n\n3) Select the \"Import\" option from the dropdown.\n\n4) At the bottom of the window that opens, click \"click here\" in \"To import a library, such as a jar or egg, click here.\"\n\n5) Drag and drop the the Hail jar from your local filespace to the appropriate place in the location. After doing so, select \"Create Library.\"\n\n6) IMPORTANT: Click the link to the jar file at the top of your screen - copy the full jar file name. You will need this later.\n\n7) Repeat step five for the pyhail.egg - there is no need to copy the name."],"metadata":{}},{"cell_type":"markdown","source":["###Getting the Cluster Up and Running\n\nHail requires some additional installation steps in order to get up and running. Specfically, we need to ensure the Hail jar is visible across the cluster. The below script creates a databricks init script which provides this visibility. In order to run it, please create a cluster (any, we're just using it to call the below script) and then run the below cell."],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.put(\"dbfs:/databricks/init/install_hail.sh\", \"\"\"\n#!/bin/bash\nJAR_NAME=\"\" # Place your JAR name here.\ncp /dbfs/FileStore/jars/${JAR_NAME} /mnt/driver-daemon/jars/hail-2_1_1.jar\ncp /dbfs/FileStore/jars/${JAR_NAME} /mnt/jars/driver-daemon/hail-2_1_1.jar\n\"\"\",\nTrue\n)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["Next, terminate the tempory cluster you just created. Create a new cluster of type `Spark 2.1 (Auto-updating, Scala 2.11) `. As well, when launching the cluster, please copy the below Spark configuration options into the Spark Tab at the bottom.\n\n```\nspark.hadoop.io.compression.codecs org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,org.apache.hadoop.io.compress.GzipCodec\nspark.sql.files.openCostInBytes 1099511627776\nspark.sql.files.maxPartitionBytes 1099511627776\nspark.hadoop.mapreduce.input.fileinputformat.split.minsize 1099511627776\nspark.hadoop.parquet.block.size 1099511627776\nspark.driver.extraClassPath ./hail-2_1_1.jar\nspark.executor.extraClassPath ./hail-2_1_1.jar\n```"],"metadata":{}},{"cell_type":"markdown","source":["### An Introduction to Hail\n\nTutorial pulled from: https://hail.is/docs/stable/tutorials/hail-overview.html\n\nWe begin with some imports."],"metadata":{}},{"cell_type":"code","source":["from hail import *\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom collections import Counter\nfrom math import log, isnan\nfrom pprint import pprint\n%matplotlib inline"],"metadata":{"collapsed":false},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Each notebook starts the same: creating a `HailContext`. The HailContext serves as an analog to Spark's `SparkContext`, and can take a SparkContext as an input.\nIn the Databricks environment, developers should utilize the already existing SparkContext `sc` and pass it into the `HailContext` method."],"metadata":{}},{"cell_type":"code","source":["hc = HailContext(sc)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Before we can read the variant dataset `vds` into Hail/Spark, we need to ensure that the data is accessible on our cluster. The below snippet checks locally for the presence of the variant dataset and fetches it if it is absent. The last three lines place the data in the databricks filestore - a cluster accessible file location."],"metadata":{}},{"cell_type":"code","source":["import os\nif os.path.isdir('/tmp/data/1kg.vds') and os.path.isfile('/tmp/data/1kg_annotations.txt'):\n    print('All files are present and accounted for!')\nelse:\n    import sys\n    sys.stderr.write('Downloading data (~50M) from Google Storage...\\n')\n    import urllib\n    import tarfile\n    urllib.urlretrieve('https://storage.googleapis.com/hail-1kg/tutorial_data.tar',\n                       '/tmp/tutorial_data.tar')\n    sys.stderr.write('Download finished!\\n')\n    sys.stderr.write('Extracting...\\n')\n    tarfile.open('/tmp/tutorial_data.tar').extractall('/tmp')\n    if not (os.path.isdir('/tmp/data/1kg.vds') and os.path.isfile('/tmp/data/1kg_annotations.txt')):\n        raise RuntimeError('Something went wrong!')\n    else:\n        sys.stderr.write('Done!\\n')\n        \ndbutils.fs.mkdirs('/FileStore/hail-data')\ndbutils.fs.cp('file:/tmp/data/1kg_annotations.txt', '/FileStore/hail-data/1kg_annotations.txt')\ndbutils.fs.cp('file:/tmp/data/1kg.vds', '/FileStore/hail-data/1kg.vds', recurse=True)\n        "],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["With the data present in our FileStore at `/FileStore/hail-data/1kg.vds`, we can go ahead and read it in. Call `vds.summarize().report()` to view some summary statistics about the variant dataset."],"metadata":{}},{"cell_type":"code","source":["vds = hc.read(\"\") # Enter in the file name.\nvds.summarize().report()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["The query_variants method is the first time we’ll see the Hail expression language. The expression language allows for a variety of incredibly expressive queries and computations, but is probably the most complex part of Hail. Try entering `'variants.take(5)'` in the method call and running the cell."],"metadata":{}},{"cell_type":"code","source":["vds.query_variants('') # Enter the query description here to view the first 5 variants."],"metadata":{"collapsed":false},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["There are often several ways to do something in Hail. Here are two ways to peek at the first few sample IDs:"],"metadata":{"collapsed":true}},{"cell_type":"code","source":["vds.query_samples('samples.take(5)')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":17},{"cell_type":"code","source":["vds.sample_ids[:5]"],"metadata":{"collapsed":false},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["There’s a similar interface for looking at the genotypes in a dataset. We use `query_genotypes` to look at the first few genotype calls."],"metadata":{}},{"cell_type":"code","source":["vds.query_genotypes('gs.take(5)')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["## Integrate Sample Annotations"],"metadata":{}},{"cell_type":"markdown","source":["The annotations file can be imported into Hail with HailContext.import_table. This method produces a KeyTable object. Think of this as a Pandas or R dataframe that isn’t limited by the memory on your machine – behind the scenes, it’s distributed with Spark."],"metadata":{}},{"cell_type":"code","source":["table = hc.import_table('/FileStore/hail-data/1kg_annotations.txt', impute=True) \\\n    .key_by('Sample')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["A good way to peek at the structure of a `KeyTable` is to look at its `schema.`"],"metadata":{}},{"cell_type":"code","source":["print(table.schema)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["The Python pprint method makes illegible printouts pretty:"],"metadata":{}},{"cell_type":"code","source":["pprint(table.schema)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["Now we’ll use this table to add sample annotations to our dataset. First, we’ll print the schema of the sample annotations already there:"],"metadata":{}},{"cell_type":"code","source":["pprint(vds.sample_schema)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["We use the annotate_samples_table method to join the table with the VDS."],"metadata":{}},{"cell_type":"code","source":["vds = vds.annotate_samples_table(table, root='sa')\npprint(vds.sample_schema)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["## Query Functions and the Hail Expression Language"],"metadata":{}},{"cell_type":"markdown","source":["We start by looking at some statistics of the information in our table. The query method uses the expression language to aggregate over the rows of the table.\n\n`counter` is an aggregation function that counts the number of occurrences of each unique element. We can use this to pull out the population distribution."],"metadata":{}},{"cell_type":"code","source":["pprint(table.query('SuperPopulation.counter()'))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["`stats` is an aggregation function that produces some useful statistics about numeric collections. We can use this to see the distribution of the CaffeineConsumption phenotype."],"metadata":{}},{"cell_type":"code","source":["pprint(table.query('CaffeineConsumption.stats()'))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["However, these metrics aren’t perfectly representative of the samples in our dataset. Here’s why:"],"metadata":{}},{"cell_type":"code","source":["vds.query_samples('samples.map(s => sa.SuperPopulation).counter()')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":38},{"cell_type":"code","source":["pprint(vds.query_samples('samples.map(s => sa.CaffeineConsumption).stats()'))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["The functionality demonstrated in the last few cells isn’t anything especially new: it’s certainly not difficult to ask these questions with Pandas or R dataframes, or even Unix tools like awk. But Hail can use the same interfaces and query language to analyze collections that are much larger, like the set of variants.\n\nHere we calculate the counts of each of the 12 possible unique SNPs (4 choices for the reference base * 3 choices for the alternate base). To do this, we need to map the variants to their alternate allele, filter to SNPs, and count by unique ref/alt pair:"],"metadata":{}},{"cell_type":"code","source":["snp_counts = vds.query_variants('variants.map(v => v.altAllele()).filter(aa => aa.isSNP()).counter()')\npprint(Counter(snp_counts).most_common())"],"metadata":{"collapsed":false},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["It’s nice to see that we can actually uncover something biological from this small dataset: we see that these frequencies come in pairs. C/T and G/A are actually the same mutation, just viewed from from opposite strands. Likewise, T/A and A/T are the same mutation on opposite strands. There’s a 30x difference between the frequency of C/T and A/T SNPs. Why?\n\nThe same Python, R, and Unix tools could do this work as well, but we’re starting to hit a wall - the latest gnomAD release publishes about 250 million variants, and that won’t fit in memory on a single computer.\n\nWhat about genotypes? Hail can query the collection of all genotypes in the dataset, and this is getting large even for our tiny dataset. Our 1,000 samples and 10,000 variants produce 10 million unique genotypes. The gnomAD dataset has about 5 trillion unique genotypes.\n\nHere we will use the hist aggregator to produce and plot a histogram of DP values for genotypes in our thousand genomes dataset."],"metadata":{}},{"cell_type":"code","source":["dp_hist = vds.query_genotypes('gs.map(g => g.dp).hist(0, 30, 30)')\nfig, ax = plt.subplots()\nplt.xlim(0, 31)\nplt.bar(dp_hist.binEdges[1:], dp_hist.binFrequencies)\ndisplay(fig)"],"metadata":{"collapsed":false,"scrolled":false},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["## Quality Control"],"metadata":{}},{"cell_type":"markdown","source":["QC is where analysts spend most of their time with sequencing datasets. QC is an iterative process, and is different for every project: there is no “push-button” solution for QC. Each time the Broad collects a new group of samples, it finds new batch effects. However, by practicing open science and discussing the QC process and decisions with others, we can establish a set of best practices as a community.\n\nQC is entirely based on the ability to understand the properties of a dataset. Hail attempts to make this easier by providing the sample_qc method, which produces a set of useful metrics as sample annotations."],"metadata":{}},{"cell_type":"code","source":["pprint(vds.sample_schema)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":46},{"cell_type":"code","source":["vds = vds.sample_qc()\npprint(vds.sample_schema)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["Interoperability is a big part of Hail. vds have a method `samples_table()` which generates a table view of the sample annotations. Initially in parquet format, this table can be converted into a pandas dataframe by calling the `to_pandas()` method on it."],"metadata":{}},{"cell_type":"code","source":["df = # Call the aforementioned methods on the variant dataset and subsequent\n     # samples table to create a pandas dataframe.\ndf.head()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["Plotting the QC metrics is a good place to start."],"metadata":{}},{"cell_type":"code","source":["plt.clf()\nplt.subplot(1, 2, 1)\nplt.hist(df[\"sa.qc.callRate\"], bins=np.arange(.75, 1.01, .01))\nplt.xlabel(\"Call Rate\")\nplt.ylabel(\"Frequency\")\nplt.xlim(.75, 1)\n\nplt.subplot(1, 2, 2)\nplt.hist(df[\"sa.qc.gqMean\"], bins = np.arange(0, 105, 5))\nplt.xlabel(\"Mean Sample GQ\")\nplt.ylabel(\"Frequency\")\nplt.xlim(0, 105)\n\nplt.tight_layout()\ndisplay(fig)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["Often, these metrics are correlated."],"metadata":{}},{"cell_type":"code","source":["plt.clf()\nplt.scatter(df[\"sa.qc.dpMean\"], df[\"sa.qc.callRate\"],\n            alpha=0.1)\nplt.xlabel('Mean DP')\nplt.ylabel('Call Rate')\nplt.xlim(0, 20)\ndisplay(fig)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["Removing outliers from the dataset will generally improve association results. We can draw lines on the above plot to indicate outlier cuts. We’ll want to remove all samples that fall in the bottom left quadrant."],"metadata":{}},{"cell_type":"code","source":["plt.scatter(df[\"sa.qc.dpMean\"], df[\"sa.qc.callRate\"],\n            alpha=0.1)\nplt.xlabel('Mean DP')\nplt.ylabel('Call Rate')\nplt.xlim(0, 20)\nplt.axhline(0.97, c='k')\nplt.axvline(4, c='k')\ndisplay(fig)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["It’s easy to filter when we’ve got the cutoff values decided:"],"metadata":{}},{"cell_type":"code","source":["vds = vds.filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97')\nprint('After filter, %d/1000 samples remain.' % vds.num_samples)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["Next is genotype QC. To start, we’ll print the post-sample-QC call rate. It’s actually gone up since the initial summary - dropping low-quality samples disproportionally removed missing genotypes."],"metadata":{}},{"cell_type":"code","source":["call_rate = vds.query_genotypes('gs.fraction(g => g.isCalled)')\nprint('pre QC call rate is %.3f' % call_rate)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["It’s a good idea to filter out genotypes where the reads aren’t where they should be: if we find a genotype called homozygous reference with >10% alternate reads, a genotype called homozygous alternate with >10% reference reads, or a genotype called heterozygote without a ref / alt balance near 1:1, it is likely to be an error."],"metadata":{}},{"cell_type":"code","source":["filter_condition_ab = '''let ab = g.ad[1] / g.ad.sum() in\n                         ((g.isHomRef && ab <= 0.1) ||\n                          (g.isHet && ab >= 0.25 && ab <= 0.75) ||\n                          (g.isHomVar && ab >= 0.9))'''\nvds = vds.filter_genotypes() # Pass in the filter condition to the\n                             # filter_genotypes() method.\npost_qc_call_rate = vds.query_genotypes('gs.fraction(g => g.isCalled)')\nprint('post QC call rate is %.3f' % post_qc_call_rate)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":["Variant QC is a bit more of the same: we can use the variant_qc method to produce a variety of useful statistics, plot them, and filter."],"metadata":{}},{"cell_type":"code","source":["pprint(vds.variant_schema)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["The cache is used to optimize some of the downstream operations."],"metadata":{}},{"cell_type":"code","source":["vds = vds.variant_qc().cache()\npprint(vds.variant_schema)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":65},{"cell_type":"code","source":["variant_df = vds.variants_table().to_pandas()\n\nplt.clf()\nplt.subplot(2, 2, 1)\nvariantgq_means = variant_df[\"va.qc.gqMean\"]\nplt.hist(variantgq_means, bins = np.arange(0, 84, 2))\nplt.xlabel(\"Variant Mean GQ\")\nplt.ylabel(\"Frequency\")\nplt.xlim(0, 80)\n\nplt.subplot(2, 2, 2)\nvariant_mleaf = variant_df[\"va.qc.AF\"]\nplt.hist(variant_mleaf, bins = np.arange(0, 1.05, .025))\nplt.xlabel(\"Minor Allele Frequency\")\nplt.ylabel(\"Frequency\")\nplt.xlim(0, 1)\n\nplt.subplot(2, 2, 3)\nplt.hist(variant_df['va.qc.callRate'], bins = np.arange(0, 1.05, .01))\nplt.xlabel(\"Variant Call Rate\")\nplt.ylabel(\"Frequency\")\nplt.xlim(.5, 1)\n\nplt.subplot(2, 2, 4)\nplt.hist(variant_df['va.qc.pHWE'], bins = np.arange(0, 1.05, .025))\nplt.xlabel(\"Hardy-Weinberg Equilibrium p-value\")\nplt.ylabel(\"Frequency\")\nplt.xlim(0, 1)\n\nplt.tight_layout()\ndisplay(fig)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":["These statistics actually look pretty good: we don’t need to filter this dataset. Most datasets require thoughtful quality control, though. The filter_variants_expr method can help!"],"metadata":{}},{"cell_type":"markdown","source":["## Let's do a GWAS!"],"metadata":{}},{"cell_type":"markdown","source":["First, we need to restrict to variants that are:\n\n* common (we’ll use a cutoff of 1%)\n* uncorrelated (not in linkage disequilibrium)\n\nBoth of these are easy in Hail.\n\nAs well, use the `count()` method tally how many sites we have remaining in `common_vds`."],"metadata":{}},{"cell_type":"code","source":["common_vds = (vds\n              .filter_variants_expr('va.qc.AF > 0.01')\n              .ld_prune(memory_per_core=256, num_cores=4))\n\ncount = # Apply the count method to common_vds().\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":["These filters removed about 15% of sites (we started with a bit over 10,000). This is NOT representative of most sequencing datasets! We have already downsampled the full thousand genomes dataset to include more common variants than we’d expect by chance.\n\nIn Hail, the association tests accept sample annotations for the sample phenotype and covariates. Since we’ve already got our phenotype of interest (caffeine consumption) in the dataset, we are good to go:"],"metadata":{}},{"cell_type":"code","source":["gwas = common_vds.linreg('sa.CaffeineConsumption')\npprint(gwas.variant_schema)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":["Looking at the bottom of the above printout, you can see the linear regression adds new variant annotations for the beta, standard error, t-statistic, and p-value."],"metadata":{}},{"cell_type":"code","source":["def qqplot(pvals, xMax, yMax):\n    spvals = sorted(filter(lambda x: x and not(isnan(x)), pvals))\n    exp = [-log(float(i) / len(spvals), 10) for i in np.arange(1, len(spvals) + 1, 1)]\n    obs = [-log(p, 10) for p in spvals]\n    fig, ax = plt.subplots()\n    plt.clf()\n    plt.scatter(exp, obs)\n    plt.plot(np.arange(0, max(xMax, yMax)), c=\"red\")\n    plt.xlabel(\"Expected p-value (-log10 scale)\")\n    plt.ylabel(\"Observed p-value (-log10 scale)\")\n    plt.xlim(0, xMax)\n    plt.ylim(0, yMax)\n    display(fig)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":74},{"cell_type":"markdown","source":["Python makes it easy to make a Q-Q (quantile-quantile) plot."],"metadata":{}},{"cell_type":"code","source":["qqplot(gwas.query_variants('variants.map(v => va.linreg.pval).collect()'),\n       5, 6)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":76},{"cell_type":"markdown","source":["## Confounded!"],"metadata":{}},{"cell_type":"markdown","source":["The observed p-values drift away from the expectation immediately. Either every SNP in our dataset is causally linked to caffeine consumption (unlikely), or there’s a confounder.\n\nWe didn’t tell you, but sample ancestry was actually used to simulate this phenotype. This leads to a stratified distribution of the phenotype. The solution is to include ancestry as a covariate in our regression.\n\nThe linreg method can also take sample annotations to use as covariates. We already annotated our samples with reported ancestry, but it is good to be skeptical of these labels due to human error. Genomes don’t have that problem! Instead of using reported ancestry, we will use genetic ancestry by including computed principal components in our model.\n\nThe pca method produces sample PCs in sample annotations, and can also produce variant loadings and global eigenvalues when asked."],"metadata":{}},{"cell_type":"code","source":["pca = common_vds.pca('sa.pca', k=5, eigenvalues='global.eigen')\npprint(pca.globals)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":79},{"cell_type":"code","source":["pprint(pca.sample_schema)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":80},{"cell_type":"markdown","source":["Now that we’ve got principal components per sample, we may as well plot them! Human history exerts a strong effect in genetic datasets. Even with a 50MB sequencing dataset, we can recover the major human populations."],"metadata":{}},{"cell_type":"code","source":["plt.clf()\nfig, ax = plt.subplots()\npca_table = pca.samples_table().to_pandas()\ncolors = {'AFR': 'green', 'AMR': 'red', 'EAS': 'black', 'EUR': 'blue', 'SAS': 'cyan'}\nplt.scatter(pca_table[\"sa.pca.PC1\"], pca_table[\"sa.pca.PC2\"],\n            c = pca_table[\"sa.SuperPopulation\"].map(colors),\n            alpha = .5)\nplt.xlim(-0.6, 0.6)\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nlegend_entries = [mpatches.Patch(color=c, label=pheno) for pheno, c in colors.items()]\nplt.legend(handles=legend_entries, loc=2)\ndisplay(fig)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":82},{"cell_type":"markdown","source":["Now we can rerun our linear regression, controlling for the first few principal components and sample sex."],"metadata":{}},{"cell_type":"code","source":["pvals = (common_vds\n        .annotate_samples_table(pca.samples_table(), expr='sa.pca = table.pca')\n        .linreg('sa.CaffeineConsumption',\n                covariates=['sa.pca.PC1', 'sa.pca.PC2', 'sa.pca.PC3', 'sa.isFemale'],\n                use_dosages=True)\n        .query_variants('variants.map(v => va.linreg.pval).collect()'))\nqqplot(pvals, 5, 6)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":84},{"cell_type":"markdown","source":["That’s more like it! We may not be publishing ten new coffee-drinking loci in Nature, but we shouldn’t expect to find anything but the strongest signals from a dataset of 1000 individuals anyway."],"metadata":{}},{"cell_type":"markdown","source":["## Rare Variant Analysis"],"metadata":{}},{"cell_type":"markdown","source":["Hail doesn’t yet have rare variant kernel-based methods, but we have linear and logistic burden tests.\n\nWe won’t be showing those here, though. Instead, we’ll demonstrate how one can use the expression language to group and count by any arbitrary properties in variant or sample annotatio"],"metadata":{}},{"cell_type":"code","source":["kt = (vds.genotypes_table()\n         .aggregate_by_key(key_expr=['pop = sa.SuperPopulation', 'chromosome = v.contig'],\n                           agg_expr=['n_het = g.filter(g => g.isHet()).count()']))\nkt.to_dataframe().show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":88},{"cell_type":"markdown","source":["What if we want to group by minor allele frequency bin and hair color, and calculate the mean GQ?\n\nWe can convert the keytable to a basic Spark DataFrame using `to_dataframe()` and then call the DataFrame's `show()` method to view the top rows."],"metadata":{}},{"cell_type":"code","source":["kt2 = (vds.genotypes_table()\n          .aggregate_by_key(key_expr=['''maf_bin = if (va.qc.AF < 0.01) \"< 1%\"\n                                                   else if (va.qc.AF < 0.05) \"1%-5%\"\n                                                   else \"> 5%\" ''',\n                                     'purple_hair = sa.PurpleHair'],\n                           agg_expr=['mean_gq = g.map(g => g.gq).stats().mean',\n                                     'mean_dp = g.map(g => g.dp).stats().mean']))\n\ndf = # Convert kt2 to a Spark DataFrame and call the show method to view\n     # its top rows."],"metadata":{"collapsed":false},"outputs":[],"execution_count":90},{"cell_type":"markdown","source":["We’ve shown that it’s easy to aggregate by a couple of arbitrary statistics. This specific examples may not provide especially useful pieces of information, but this same pattern can be used to detect effects of rare variation:\n\nCount the number of heterozygous genotypes per gene by functional category (synonymous, missense, or loss-of-function) to estimate per-gene functional constraint\nCount the number of singleton loss-of-function mutations per gene in cases and controls to detect genes involved in disease"],"metadata":{}}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2},"version":"2.7.13","nbconvert_exporter":"python","file_extension":".py"},"name":"hail_tutorial_INSTRUCTOR","notebookId":706959050890843,"kernelspec":{"display_name":"Hail","language":"python","name":"hail"},"anaconda-cloud":{}},"nbformat":4,"nbformat_minor":0}
